{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded forecast tutorial - Italy 2010 Experiment\n",
    "\n",
    "In this tutorial, we will load and test grid-based forecasts and interpret the results of the tests provided in the PyCSEP package for gridded forecasts. We will work with two time-independent grid-based forecasts submitted as part of the CSEP Italy testing experiment (see [Werner et al, 2010](https://doi.org/10.4401/ag-4840), [Taroni et al, 2018](https://doi.org/10.1785/0220180031) for some previous testing results). Our goal is to compare the performance of these two forecasts for describing observed Italian seismicity.   \n",
    "\n",
    "This is essentially a three step process:  \n",
    "    1. Read in (and plot) a gridded forecast  \n",
    "    2. Set up an evaluation catalog of observed events  \n",
    "    3. Run PyCSEP tests and interpret the results    \n",
    "\n",
    "\n",
    "We introduce the concepts to the reader and encourage them to explore the other tests available. Full documentation of the package can be found [here](https://docs.cseptesting.org/) and any issues can be reported on the [PyCSEP Github page](https://github.com/SCECcode/pycsep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functionalities are found at the top-level of the csep package\n",
    "import csep\n",
    "# The testing core classes and routines are found in the csep.core sub-module\n",
    "from csep.core import regions, poisson_evaluations\n",
    "# Utilities are available from the csep.utils sub-module.\n",
    "from csep.utils import datasets, time_utils, comcat, plots, readers\n",
    "\n",
    "import numpy\n",
    "import cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by setting up some experiment parameters. It is good practice to set this up early. Note, the start and end date of the forecast should be chosen based on the creation of the forecast. This is important for time-independent forecasts because they can be rescaled to any arbitrary time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up experiment parameters\n",
    "start_date = time_utils.strptime_to_utc_datetime('2010-01-01 00:00:00.0')\n",
    "end_date = time_utils.strptime_to_utc_datetime('2015-01-01 00:00:00.0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load two gridded forecasts found in `workshop_data/forecasts` in the standard CSEP `.dat` format. You can learn more about the multiple accepted and custom formats [in the documentation](https://docs.cseptesting.org/concepts/forecasts.html). An example of the `.dat`format is shown below.\n",
    "```\n",
    "# min_lat | max_lat | min_lon | max_lon | min_depth | max_depth | min_mag | max_mag | rate | mask\n",
    "5.50\t5.60\t44.90\t45.00\t0.00\t30.00\t4.95\t5.05\t3.3000000000000003e-05\t1\n",
    "5.50\t5.60\t44.90\t45.00\t0.00\t30.00\t5.05\t5.15\t2.5999999999999998e-05\t1\n",
    "5.50\t5.60\t44.90\t45.00\t0.00\t30.00\t5.15\t5.25\t2.0000000000000002e-05\t1\n",
    "...\n",
    "5.50\t5.60\t45.00\t45.10\t0.00\t30.00\t4.95\t5.05\t3.3000000000000003e-05\t1\n",
    "5.50\t5.60\t45.00\t45.10\t0.00\t30.00\t5.05\t5.15\t2.5999999999999998e-05\t1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Resolution Smoothed Seismicity Model 1 - [Werner et al., 2010](https://doi.org/10.4401/ag-4839)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads from the PyCSEP package\n",
    "werner_forecast = csep.load_gridded_forecast('../workshop_data/forecasts/HRSS_m1.dat',\n",
    "                                             name='Werner, et al (2010)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seismicity forecast from the 2004 Italian Seismic Hazard Model  - [Meletti et al., 2004](http://zonesismiche.mi.ingv.it/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You may need to edit the file location here depending on your set up\n",
    "meletti_forecast = csep.load_gridded_forecast(\"../workshop_data/forecasts/MPS04after.dat\",\n",
    "                                              name =\"Meletti et al (2010), MPS working group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridded forecasts inherit the region from the forecast, so there is no requirement to explicitly set this. We should check, however, that the forecast regions for catalogs we want to compare are the same so that they are testable with a single catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check - if forecasts have the same region this will provide no output.\n",
    "numpy.testing.assert_allclose(meletti_forecast.region.midpoints(), werner_forecast.region.midpoints())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise this forecast, we will use `forecast.plot()` with some specifications to get a nicer looking figure. We will do this by creating a dictionary containing the plot arguments.  \n",
    "\n",
    "These arguments are, in order:\n",
    "\n",
    "    - Assign a title\n",
    "    - Set labels to the geographic axes\n",
    "    - Draw country borders\n",
    "    - Set a linewidth of 0.5 to country borders\n",
    "    - Select ESRI Imagery as a basemap.\n",
    "    - Assign 'rainbow' as colormap. Possible values from from matplotlib.cm library\n",
    "    - Defines 0.8 for an exponential transparency function (default is 0 for constant alpha, whereas 1 for linear).\n",
    "    - An object cartopy.crs.Projection() is passed as Projection to the map  \n",
    " \n",
    " The complete description of plot arguments can be found in `csep.utils.plots.plot_spatial_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {'title': 'Italy 10 year forecast',\n",
    "             'grid_labels': True,\n",
    "             'borders': True,\n",
    "             'feature_lw': 0.5,\n",
    "             'basemap': 'ESRI_imagery',\n",
    "             'cmap': 'rainbow',\n",
    "             'alpha_exp': 0.8,\n",
    "             'projection': cartopy.crs.Mercator()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map extent can also be defined. Otherwise, the extent of the data would be used. The dictionary defined must be passed as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = werner_forecast.plot(extent=[3, 22, 35, 48],\n",
    "                          show=True,\n",
    "                          plot_args=args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the second forecast\n",
    "# An exercise for the reader\n",
    "ax = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up evaluation catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to import the observed catalog that we want to use to test the forecast - we call this the **evaluation catalog**.  \n",
    "\n",
    "There are multiple ways to include evaluation catalogs, including accessing directly to catalog providers, such as ComCat for US models, GeoNet for New Zealand, BSI for Italy, etc. There are also various readers currently included with the package, including those for JMA and the INGV HORUS catalogs, which are described [in the documentation](https://docs.cseptesting.org/concepts/catalogs.html#loading-catalogs). These functions are found in `csep/utils/readers.py` if you would like to see them or understand how to add your own.  \n",
    "\n",
    "In this case we demonstrate by querying the API of the Bollettino Seismico Italiano (INGV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## query catalog from provider\n",
    "italy_test_catalog = csep.query_bsi(start_time = start_date, end_time=end_date)\n",
    "\n",
    "## In case the INGV API cannot be accesed, load the catalog from the workshop material\n",
    "# italy_test_catalog = csep.load_catalog(\"../workshop_data/catalogs/italy_bsi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the catalog to check the range of dates, locations and magnitudes of the events in the evaluation catalog, as well as the total number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An exercise for the reader\n",
    "print(dir(italy_test_catalog))  # print the catalog available methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter the catalog to the desired time-space-magnitude range. Crucially, we must also filter the catalog to the forecast region in order to carry out any testing in the next step. This is also why we checked that the forecasts we want to compare are in the same spatial region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_test_catalog = italy_test_catalog.filter_spatial(werner_forecast.region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our magnitude range is not consistent with the parameters we established earlier, so we have to filter for magnitude also. This is obviously important to fairly test a forecast, and you can see why if you re-run this tutorial without this step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_test_catalog = italy_test_catalog.filter('magnitude >= 4.95')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run the above code, you should be left with a catalog of 13 events. Print the catalog with the standard python `print` command to check the range of dates, locations and magnitudes of the events in the evaluation catalog, as well as the total number of events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to answer some questions about our forecasts and their performance. In this example, we will investigate the spatial properties of the forecast models and how well the forecasts describe the observed spatial distribution of seismicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consistency tests implemented for gridded forecasts in PyCSEP are the N, S and M-test described in [Schorlemmer et al, 2007](https://doi.org/10.1785/gssrl.78.1.17) and [Zechar et al, 2010](https://doi.org/10.1785/0120090192). These are located in the `poisson_evaluations` file that we have imported as `poisson`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out a test, we simply provide the forecast we wish to test and an evaluation forecast. The spatial test requires simulating from the Poisson forecast to provide uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_test_result_werner = poisson_evaluations.spatial_test(werner_forecast, italy_test_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat the spatial test for our second example forecast\n",
    "spatial_test_result_meletti = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCSEP provides easy ways of storing objects to a JSON format using csep.write_json(). The evaluations can be read back into the program for plotting using `csep.load_evaluation_result()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to write to .json file (optional)\n",
    "## You can look at the contents of this file in jupyter lab to see how the data is stored\n",
    "csep.write_json(spatial_test_result_meletti, 'example_spatial_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these results using the `plot_poisson_consistency_test` function from the `plots` file of `csep.utils`, where you can find more details on the plot arguments or [in the documentation](https://docs.cseptesting.org/reference/generated/csep.utils.plots.plot_poisson_consistency_test.html#csep.utils.plots.plot_poisson_consistency_test). Again, we use a dictionary to set up some plot arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'figsize': (6,5),\n",
    "        'title': r'$\\mathcal{S}-\\mathrm{test}$',\n",
    "        'title_fontsize': 18,\n",
    "        'xlabel': 'Log-likelihood',\n",
    "        'xticks_fontsize': 12,\n",
    "        'ylabel_fontsize': 12,\n",
    "        'linewidth': 1,\n",
    "        'capsize': 4,\n",
    "        'hbars':True,\n",
    "        'tight_layout': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to plot the results of both forecasts for comparison. We set `one_sided_lower=True` as usual for an L-test, where the model is rejected if the observed is located within the lower tail of the simulated distribution. We can supply multiple `spatial_test_result` objects in a list (specified in the square brackets as standard in python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plots.plot_poisson_consistency_test([spatial_test_result_werner, spatial_test_result_meletti],\n",
    "                                         one_sided_lower=True, plot_args=args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this process for N or M tests using the `number_test` or `magnitude_test` from `poisson_evaluations` if we are more interested in these components specifically, or getting a fuller picture of where the forecast does well and not so well.   \n",
    "Try out a `likelihood_test` or `conditional_likelihood_test` (also from `poisson_evaluations`). What does this tell you about the two forecasts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should this result be interpreted? Are there other tests that might be useful here? What are your overall conclusions on the performance of these two forecasts?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are happy with all of this so far, you can try:  \n",
    "* Check the [theory](https://docs.cseptesting.org/getting_started/theory.html) and the [implementation](https://docs.cseptesting.org/concepts/evaluations.html) of additional testing methods \n",
    "* Changing the time period for the evaluation    \n",
    "* Filtering the evaluation catalog by a different time period or magnitude    \n",
    "* Using PyCSEP gridded-forecast tests on your own forecast models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "\n",
    "Schorlemmer, D., M. Gerstenberger, S. Wiemer, D. D. Jackson, and D. A. Rhoades (2007). Earthquake likelihood model testing, Seismological Research Letters 78 17-29.  \n",
    "Taroni, M., W. Marzocchi, D. Schorlemmer, M. J. Werner, S. Wiemer, J. D. Zechar, L. Heiniger, F. Euchner (2018). Prospective CSEP Evaluation of 1‐Day, 3‐Month, and 5‐Yr Earthquake Forecasts for Italy, Seismological Research Letters (2018) 89 (4): 1251–1261  \n",
    "Werner, M. J., J. D. Zechar, W. Marzocchi, S., CSEP-Italy Working Group (2010). Retrospective evaluation of the five-year and ten-year CSEP-Italy earthquake forecasts, Annals of Geophysics, 53, 3.  \n",
    "Zechar, J. D., M. C. Gerstenberger, and D. A. Rhoades (2010). Likelihood-Based Tests for Evaluating Space-Rate-Magnitude Earthquake Forecasts, Bulletin of the Seismological Society of America 100 1184-1195."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
